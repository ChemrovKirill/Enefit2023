{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7386434,"sourceType":"datasetVersion","datasetId":4293331},{"sourceId":7514419,"sourceType":"datasetVersion","datasetId":4376935},{"sourceType":"datasetVersion","sourceId":7516127,"datasetId":4378021}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index --find-links /kaggle/input/pvlib-dependency/package pvlib","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:20:35.415263Z","iopub.execute_input":"2024-01-31T23:20:35.415614Z","iopub.status.idle":"2024-01-31T23:20:48.376321Z","shell.execute_reply.started":"2024-01-31T23:20:35.415586Z","shell.execute_reply":"2024-01-31T23:20:48.375177Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/pvlib-dependency/package\nProcessing /kaggle/input/pvlib-dependency/package/pvlib-0.10.3-py3-none-any.whl\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from pvlib) (1.24.3)\nRequirement already satisfied: pandas>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from pvlib) (2.0.3)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from pvlib) (2023.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pvlib) (2.31.0)\nRequirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from pvlib) (1.11.4)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from pvlib) (3.9.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.0->pvlib) (2.8.2)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.0->pvlib) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pvlib) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pvlib) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pvlib) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pvlib) (2023.11.17)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.0->pvlib) (1.16.0)\nInstalling collected packages: pvlib\nSuccessfully installed pvlib-0.10.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations, product  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n# üì¶ Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\nfrom catboost import CatBoostRegressor, Pool\nfrom catboost import EShapCalcType, EFeaturesSelectionAlgorithm\n\nfrom tqdm.notebook import tqdm  # Progress bar\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numba\nimport plotly.express as px\nimport math\nimport itertools\nimport holidays\nimport pvlib\n\n# ü§ê Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:20:48.378224Z","iopub.execute_input":"2024-01-31T23:20:48.378549Z","iopub.status.idle":"2024-01-31T23:20:53.409721Z","shell.execute_reply.started":"2024-01-31T23:20:48.378525Z","shell.execute_reply":"2024-01-31T23:20:53.408479Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# switching NB mode\nIS_OFFLINE = False\nIS_SUBMIT = True\nIS_TRAIN = False\nIS_GENFEATS = True\nK_FOLD = False\n\nIS_CONS_MODEL = False\nIS_CONS_DIFF_MODEL = True\nIS_CONS_DIFF7_MODEL = False\nweights2name = {'cons-0':0, 'cons-1':0,\n                'diff-cons-0':1, 'diff-cons-1':0.5,\n                'diff7-cons-0':0, 'diff7-cons-1':0,}\n\nsplit_data_block = 500\nif IS_OFFLINE:\n    mode = f'offline-{split_data_block}'\nelse:\n    mode = 'online'","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:20:53.410932Z","iopub.execute_input":"2024-01-31T23:20:53.411297Z","iopub.status.idle":"2024-01-31T23:20:53.419490Z","shell.execute_reply.started":"2024-01-31T23:20:53.411266Z","shell.execute_reply":"2024-01-31T23:20:53.418113Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n# if not IS_SUBMIT:\ndf_data = pd.read_csv(os.path.join(root, \"train.csv\"))\ndf_client = pd.read_csv(os.path.join(root, \"client.csv\"))\ndf_gas_prices = pd.read_csv(os.path.join(root, \"gas_prices.csv\"))\ndf_electricity_prices = pd.read_csv(os.path.join(root, \"electricity_prices.csv\"))\ndf_forecast_weather = pd.read_csv(os.path.join(root, \"forecast_weather.csv\"))\ndf_historical_weather = pd.read_csv(os.path.join(root, \"historical_weather.csv\"))\ndf_weather_station_to_county_mapping = pd.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:20:53.422014Z","iopub.execute_input":"2024-01-31T23:20:53.422348Z","iopub.status.idle":"2024-01-31T23:21:14.368737Z","shell.execute_reply.started":"2024-01-31T23:20:53.422295Z","shell.execute_reply":"2024-01-31T23:21:14.367672Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"def plot_data_groupby(df, x, y, groupby):\n    if 'date' in x:\n        df[x] = pd.to_datetime(df[x])\n    df = df.groupby(groupby)\n    num_of_groups = len(df)\n    n_col = 4\n    n_row = math.ceil(num_of_groups / n_col)\n    fig, axs = plt.subplots(n_row, n_col, figsize=(24, 4*n_row))\n    if len(axs.shape) == 1:\n        axs = axs.reshape(1, axs.shape[0])\n    print(axs.shape)\n    for i, (group_id, values) in enumerate(df[y]):\n        ax = axs[i // 4, i % 4]\n        ax.plot(values)\n        ax.set_title(str(groupby) + ': ' + str(group_id) + ' | n=' + str(len(values)), fontsize=10)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.369988Z","iopub.execute_input":"2024-01-31T23:21:14.370267Z","iopub.status.idle":"2024-01-31T23:21:14.378260Z","shell.execute_reply.started":"2024-01-31T23:21:14.370245Z","shell.execute_reply":"2024-01-31T23:21:14.376744Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"class ClientCompiler:\n    def __init__(self, days_to_save=30):\n        self.days_to_save = days_to_save\n        all_codes = []\n        is_business = [0, 1]\n        product_type = [0, 1, 2, 3]\n        county = list(range(16))\n        for comb in itertools.product(county, product_type, is_business):\n            all_codes.append(f'C_{comb[0]}|PT_{comb[1]}|B_{comb[2]}')\n        all_columns = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business']\n        self.client_last = pd.DataFrame({'product_type': pd.Series(dtype='int'),\n                                         'county': pd.Series(dtype='int'),\n                                         'eic_count': pd.Series(dtype='float'),\n                                         'installed_capacity': pd.Series(dtype='float'), \n                                         'is_business': pd.Series(dtype='int')}, \n                                        index=all_codes)\n        self.client_relevant = pd.DataFrame({'product_type': pd.Series(dtype='int'),\n                                             'county': pd.Series(dtype='int'),\n                                             'eic_count': pd.Series(dtype='float'),\n                                             'installed_capacity': pd.Series(dtype='float'), \n                                             'is_business': pd.Series(dtype='int'),\n                                             'date': pd.Series(dtype='object'),\n                                            })\n    \n    def update(self, client_block):\n        cols_to_int = ['is_business', 'product_type', 'county']\n        client_block[cols_to_int] = client_block[cols_to_int].astype(int)\n        date_value = pd.to_datetime(client_block['date'].iloc[0])\n        client_block['code'] = client_block.apply(lambda x: f'C_{x[\"county\"]}|PT_{x[\"product_type\"]}|B_{x[\"is_business\"]}', axis=1)\n        client_block = client_block.set_index('code').drop(columns=['date', 'data_block_id'], errors='ignore')\n        self.client_last.update(client_block)\n        client_block['date'] = date_value\n        self.client_relevant = pd.concat([self.client_relevant, client_block], ignore_index=True)\n        self.client_relevant.drop(self.client_relevant[\n            self.client_relevant['date'] <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)    \n            \n    def add_client(self, data):\n        data = pd.merge(data, self.client_last, on=['county', 'product_type', 'is_business'], how='left')\n        return data\n    \n    def add_client_daylag(self, data, days):\n        client_block = self.client_relevant.copy()\n        client_block['date']  = client_block['date'] + pd.Timedelta(days=days)\n        data['date'] = pd.to_datetime(data['datetime'].apply(lambda x: x.date()))\n        data = pd.merge(data, client_block, on=['date', 'county', 'product_type', 'is_business'], how='left', suffixes=['', f'__lag_{days}d'])\n        data = data.drop(columns=['date'])\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.379805Z","iopub.execute_input":"2024-01-31T23:21:14.380111Z","iopub.status.idle":"2024-01-31T23:21:14.400550Z","shell.execute_reply.started":"2024-01-31T23:21:14.380090Z","shell.execute_reply":"2024-01-31T23:21:14.399498Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class HistoricalWeatherCompiler:\n    def __init__(self, days_to_save=30):\n        self.days_to_save = days_to_save\n        self.historical_relevant = pd.DataFrame(columns=[\n                'county', 'datetime', 'temperature', 'dewpoint', 'snowfall', 'cloudcover_high', \n                 'cloudcover_mid', 'cloudcover_low', 'cloudcover_total',\n                 'windspeed_total', 'u_wind_component', 'v_wind_component', 'winddirection',\n                 'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation', \n                 'surface_pressure', 'rain', \n                 'solar_angle_hor_cos', 'solar_angle_ideal_cos'\n                ])\n        \n        self.historical_allsta = pd.DataFrame(columns=[\n                 'datetime', 'temperature', 'dewpoint', 'snowfall', 'cloudcover_high', \n                 'cloudcover_mid', 'cloudcover_low', 'cloudcover_total',\n                 'windspeed_total', 'u_wind_component', 'v_wind_component', 'winddirection',\n                 'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation', \n                 'surface_pressure', 'rain', \n                 'solar_angle_hor_cos', 'solar_angle_ideal_cos'\n                ])\n        \n        self.county_mapping = df_weather_station_to_county_mapping[['longitude', 'latitude', 'county']]\n        self.county_mapping['longitude'] = (self.county_mapping['longitude'] * 10).round(0).astype(int)\n        self.county_mapping['latitude'] = (self.county_mapping['latitude'] * 10).round(0).astype(int)\n        \n    def update(self, historical_block):\n        date_value = pd.to_datetime(historical_block['datetime'].iloc[0])\n        \n        # HISTORICAL\n        historical_block['datetime'] = pd.to_datetime(historical_block['datetime'])\n        cols = pvlib.solarposition.get_solarposition(historical_block['datetime'], \n                                                     historical_block['latitude'], \n                                                     historical_block['longitude'], \n                                                     temperature=historical_block['temperature']\n                                                    )[['zenith', 'azimuth']].reset_index()\n        historical_block['solar_angle_hor_cos'] = np.cos(np.radians(pvlib.irradiance.aoi(0, 180, cols['zenith'], cols['azimuth']).values))\n        historical_block['solar_angle_ideal_cos'] =  np.cos(np.radians(pvlib.irradiance.aoi(41, 180, cols['zenith'], cols['azimuth']).values))\n        \n        historical_block['longitude'] = (historical_block['longitude'] * 10).round(0).astype(int)\n        historical_block['latitude'] = (historical_block['latitude'] * 10).round(0).astype(int)\n        historical_block = historical_block.merge(self.county_mapping, on=['longitude', 'latitude'], how='left')\n        historical_block['windspeed_total'] = historical_block['windspeed_10m']\n        historical_block['u_wind_component'] = historical_block['windspeed_10m'] * np.sin(historical_block['winddirection_10m'] / 180 * np.pi)\n        historical_block['v_wind_component'] = historical_block['windspeed_10m'] * np.cos(historical_block['winddirection_10m'] / 180 * np.pi)\n        historical_block['winddirection'] = np.sin(historical_block['winddirection_10m'] / 180 * np.pi)\n        \n        historical_block_avg = historical_block.drop(columns=['county']).groupby(['datetime']).mean().reset_index()\n        historical_block = historical_block.groupby(['county', 'datetime']).mean().reset_index() #  ==========\n        \n        historical_block_avg = historical_block_avg.drop(columns=['latitude', 'longitude', 'windspeed_10m', 'winddirection_10m', 'data_block_id'],\n                                                 errors='ignore')\n        historical_block = historical_block.drop(columns=['latitude', 'longitude', 'windspeed_10m', 'winddirection_10m', 'data_block_id'],\n                                                 errors='ignore')\n        \n        self.historical_allsta = pd.concat([self.historical_allsta, historical_block_avg], ignore_index=True)\n        self.historical_allsta.drop(self.historical_allsta[\n            self.historical_allsta['datetime'] <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)\n                                    \n        \n        self.historical_relevant = pd.concat([self.historical_relevant, historical_block], ignore_index=True)\n        self.historical_relevant.drop(self.historical_relevant[\n            self.historical_relevant['datetime'] <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)\n        \n    def add_weather_daylag(self, data, days, feats=None): #  historical data is used\n        assert days >= 1          \n        if feats is None:\n            feats = list(self.historical_relevant.columns)\n        else:\n            feats = list(set(['datetime','county'] + feats))\n        historical_block = self.historical_relevant[feats]\n        if days == 1:\n            historical_block = historical_block[historical_block['datetime'].apply(lambda x: x.hour) <= 10]\n        historical_block['datetime'] = historical_block['datetime'] + pd.Timedelta(days=days)\n        data = pd.merge(data, historical_block, on=['county', 'datetime'], how='left', suffixes=['', f'__lag_{days}d'])\n        data.rename(columns={'shortwave_radiation': 'shortwave_radiation' + f'__lag_{days}d',\n                             'diffuse_radiation': 'diffuse_radiation' + f'__lag_{days}d',\n                             'surface_pressure': 'surface_pressure' + f'__lag_{days}d',\n                             'rain': 'rain' + f'__lag_{days}d'}, \n                    inplace=True)\n        return data\n    \n    def add_weather_allsta_daylag(self, data, days, feats=None): #  historical data is used\n        assert days >= 1\n        if feats is None:\n            feats = list(self.historical_allsta.columns)\n        else:\n            feats = list(set(['datetime'] + feats))\n        historical_block = self.historical_allsta[feats]\n        if days == 1:\n            historical_block = historical_block[historical_block['datetime'].apply(lambda x: x.hour) <= 10]\n        historical_block['datetime'] = historical_block['datetime'] + pd.Timedelta(days=days)\n        data = pd.merge(data, historical_block, on=['datetime'], how='left', suffixes=['', f'__lag_{days}d_allsta'])\n        data.rename(columns={'shortwave_radiation': 'shortwave_radiation' + f'__lag_{days}d_allsta',\n                             'diffuse_radiation': 'diffuse_radiation' + f'__lag_{days}d_allsta',\n                             'surface_pressure': 'surface_pressure' + f'__lag_{days}d_allsta',\n                             'rain': 'rain' + f'__lag_{days}d_allsta'}, \n                    inplace=True)\n        return data\n    \n    def add_weather_daylag_daymean(self, data, days, feats=None):\n        assert days >= 2\n        if feats is None:\n            feats = list(self.historical_relevant.columns)\n        else:\n            feats = list(set(['datetime','county'] + feats))\n        historical_block = self.historical_relevant[feats]\n        historical_block['datetime'] = historical_block['datetime'] + pd.Timedelta(days=days)\n        historical_block['date'] = historical_block['datetime'].apply(lambda x: x.date())\n        historical_block = historical_block.drop(columns=['datetime'])\n        historical_block = historical_block.groupby(['date', 'county']).mean().reset_index()\n        data['date'] = data['datetime'].apply(lambda x: x.date())\n        data = pd.merge(data, historical_block, on=['county', 'date'], how='left', suffixes=['', f'__meanlag_{days}d']).drop(columns=['date'])\n        data.rename(columns={'shortwave_radiation': 'shortwave_radiation' + f'__meanlag_{days}d',\n                             'diffuse_radiation': 'diffuse_radiation' + f'__meanlag_{days}d',\n                             'surface_pressure': 'surface_pressure' + f'__meanlag_{days}d',\n                             'rain': 'rain' + f'__meanlag_{days}d'}, \n                    inplace=True)\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.402188Z","iopub.execute_input":"2024-01-31T23:21:14.402527Z","iopub.status.idle":"2024-01-31T23:21:14.430756Z","shell.execute_reply.started":"2024-01-31T23:21:14.402502Z","shell.execute_reply":"2024-01-31T23:21:14.429363Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ForecastedWeatherCompiler:\n    def __init__(self, days_to_save=30):\n        self.days_to_save = days_to_save\n        \n        self.forecasted_relevant = pd.DataFrame({   'datetime': pd.Series(dtype='object'),\n                                                    'county': pd.Series(dtype='float'),\n                                                    'temperature': pd.Series(dtype='float'),\n                                                    'dewpoint': pd.Series(dtype='float'),\n                                                    'cloudcover_high': pd.Series(dtype='float'),\n                                                    'cloudcover_low': pd.Series(dtype='float'),\n                                                    'cloudcover_mid': pd.Series(dtype='float'),\n                                                    'cloudcover_total': pd.Series(dtype='float'),\n                                                    'direct_solar_radiation': pd.Series(dtype='float'),\n                                                    'surface_solar_radiation_downwards': pd.Series(dtype='float'),\n                                                    'snowfall': pd.Series(dtype='float'),\n                                                    'total_precipitation': pd.Series(dtype='float'),\n                                                    'windspeed_total': pd.Series(dtype='float'),\n                                                    'u_wind_component': pd.Series(dtype='float'),\n                                                    'v_wind_component': pd.Series(dtype='float'),\n                                                    'winddirection': pd.Series(dtype='float'),\n                                                    'solar_angle_hor_cos': pd.Series(dtype='float'),\n                                                    'solar_angle_ideal_cos': pd.Series(dtype='float'),\n                                                }) #  'origin_datetime', 'hours_ahead'\n        \n        self.forecasted_allsta = self.forecasted_relevant.drop(columns=['county'])\n        \n        self.county_mapping = df_weather_station_to_county_mapping[['longitude', 'latitude', 'county']]\n        self.county_mapping['longitude'] = (self.county_mapping['longitude'] * 10).round(0).astype(int)\n        self.county_mapping['latitude'] = (self.county_mapping['latitude'] * 10).round(0).astype(int)\n        \n    def update(self, forecasted_block, do_mean=False):\n        date_value = pd.to_datetime(forecasted_block['origin_datetime'].iloc[0])\n        \n        # FORECASTED\n        forecasted_block['datetime'] = pd.to_datetime(forecasted_block['forecast_datetime'])\n        date_value = pd.to_datetime(forecasted_block['datetime'].iloc[0])\n        \n        cols = pvlib.solarposition.get_solarposition(forecasted_block['datetime'], \n                                                     forecasted_block['latitude'], \n                                                     forecasted_block['longitude'], \n                                                     temperature=forecasted_block['temperature']\n                                                    )[['zenith', 'azimuth']].reset_index()  # indexes = datetime\n        forecasted_block['solar_angle_hor_cos'] = np.cos(np.radians(pvlib.irradiance.aoi(0, 180, cols['zenith'], cols['azimuth']).values))\n        forecasted_block['solar_angle_ideal_cos'] =  np.cos(np.radians(pvlib.irradiance.aoi(41, 180, cols['zenith'], cols['azimuth']).values))\n        \n        forecasted_block['longitude'] = (forecasted_block['longitude'] * 10).round(0).astype(int)\n        forecasted_block['latitude'] = (forecasted_block['latitude'] * 10).round(0).astype(int)\n        forecasted_block = forecasted_block.merge(self.county_mapping, on=['longitude', 'latitude'], how='left')\n#         forecasted_block = forecasted_block.dropna(subset=['county'])\n        forecasted_block['windspeed_total'] = (forecasted_block['10_metre_u_wind_component']**2 + forecasted_block['10_metre_v_wind_component']**2)**0.5\n        forecasted_block['u_wind_component'] = forecasted_block['10_metre_u_wind_component']\n        forecasted_block['v_wind_component'] = forecasted_block['10_metre_v_wind_component']\n        forecasted_block['winddirection'] = forecasted_block['10_metre_u_wind_component'] / forecasted_block['windspeed_total']\n        if not do_mean:\n            forecasted_block = forecasted_block[(forecasted_block['hours_ahead'] >= 22) & (forecasted_block['hours_ahead'] < 46)]\n        forecasted_block = forecasted_block.drop(columns=['latitude', 'longitude', 'forecast_datetime', 'origin_datetime', \n                                                          '10_metre_u_wind_component', '10_metre_v_wind_component', 'data_block_id'], \n                                                 errors='ignore')\n        forecasted_block = forecasted_block.drop(columns=['hours_ahead'])\n        forecasted_block_avg = forecasted_block.drop(columns=['county']).groupby(['datetime']).mean().reset_index() #  ==========\n        forecasted_block = forecasted_block.groupby(['county', 'datetime']).mean().reset_index() #  ==========\n\n        \n\n        self.forecasted_relevant = pd.concat([self.forecasted_relevant, forecasted_block], ignore_index=True)\n#         self.forecasted_relevant = self.forecasted_relevant.groupby(['county', 'datetime']).mean().reset_index() #  ==========\n        self.forecasted_relevant.drop(self.forecasted_relevant[\n            self.forecasted_relevant['datetime'] <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)\n        \n        self.forecasted_allsta = pd.concat([self.forecasted_allsta, forecasted_block_avg], ignore_index=True)\n#         self.forecasted_relevant = self.forecasted_relevant.groupby(['county', 'datetime']).mean().reset_index() #  ==========\n        self.forecasted_allsta.drop(self.forecasted_allsta[\n            self.forecasted_allsta['datetime'] <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)\n        \n    def add_weather(self, data) :\n        return self.add_weather_hourlag(data, 0)\n    \n    def add_weather_allsta(self, data) :\n        return self.add_weather_allsta_hourlag(data, 0)\n        \n    def add_weather_hourlag(self, data, hours, feats=None): #  forecasted data is used\n        if feats is None:\n            feats = list(self.forecasted_relevant.columns)\n        else:\n            feats = list(set(['datetime','county'] + feats))\n        forecasted_block = self.forecasted_relevant[feats]\n        forecasted_block['datetime'] = forecasted_block['datetime'] + pd.Timedelta(hours=hours)\n        return pd.merge(data, forecasted_block, on=['county', 'datetime'], how='left', suffixes=['', f'__lag_{hours}h'])\n    \n    def add_weather_allsta_hourlag(self, data, hours, feats=None): #  forecasted data is used\n        if feats is None:\n            feats = list(self.forecasted_allsta.columns)\n        else:\n            feats = list(set(['datetime'] + feats))\n        forecasted_block = self.forecasted_allsta[feats]\n        forecasted_block['datetime'] = forecasted_block['datetime'] + pd.Timedelta(hours=hours)\n        return pd.merge(data, forecasted_block, on=['datetime'], how='left', suffixes=['', f'__lag_{hours}h_allsta'])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.432053Z","iopub.execute_input":"2024-01-31T23:21:14.432465Z","iopub.status.idle":"2024-01-31T23:21:14.455611Z","shell.execute_reply.started":"2024-01-31T23:21:14.432433Z","shell.execute_reply":"2024-01-31T23:21:14.453789Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class GasCompiler:\n    def __init__(self, days_to_save=30):\n        self.days_to_save = days_to_save\n        self.gas_relevant = pd.DataFrame(columns=['date', 'gas_price_low', 'gas_price_high'])\n    \n    def update(self, gas_block):\n        gas_block = gas_block.iloc[[-1]].drop(columns=['origin_date', 'data_block_id'],\n                                              errors='ignore')\n        gas_block = gas_block.rename(columns={'forecast_date': 'date',\n                                              'lowest_price_per_mwh': 'gas_price_low',\n                                              'highest_price_per_mwh': 'gas_price_high'})\n        self.gas_relevant = pd.concat([self.gas_relevant, gas_block], ignore_index=True)\n        date_value = pd.to_datetime(gas_block['date'].iloc[0])\n        self.gas_relevant.drop(self.gas_relevant[\n            pd.to_datetime(self.gas_relevant['date']) <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)\n        \n    \n    def add_gas(self, data):\n        return self.add_gas_daylag(data, 0)\n    \n    def add_gas_daylag(self, data, days):\n        col_names = [f'gas_price_low__lag_{days}d', f'gas_price_high__lag_{days}d']\n        if days < len(self.gas_relevant):   \n            data[col_names[0]] = self.gas_relevant.iloc[-days-1, :]['gas_price_low']\n            data[col_names[1]] = self.gas_relevant.iloc[-days-1, :]['gas_price_high']\n        else:\n            data[col_names[0]] = np.nan\n            data[col_names[1]] = np.nan\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.457360Z","iopub.execute_input":"2024-01-31T23:21:14.457800Z","iopub.status.idle":"2024-01-31T23:21:14.475848Z","shell.execute_reply.started":"2024-01-31T23:21:14.457767Z","shell.execute_reply":"2024-01-31T23:21:14.474738Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ElectricityCompiler:\n    def __init__(self, days_to_save=30):\n        self.days_to_save = days_to_save\n        self.electricity_relevant = pd.DataFrame(columns=['date', 'electricity_price'])\n    \n    def update(self, electricity_block):\n        electricity_block = electricity_block.drop(columns=['origin_date', 'data_block_id'], \n                                                   errors='ignore')\n        electricity_block['euros_per_mwh'] = electricity_block['euros_per_mwh'].mean()\n        electricity_block = electricity_block.iloc[[-1]].rename(columns={'forecast_date': 'date',\n                                                                          'euros_per_mwh': 'electricity_price'})\n        self.electricity_relevant = pd.concat([self.electricity_relevant, electricity_block], ignore_index=True)\n        date_value = pd.to_datetime(electricity_block['date'].iloc[0])\n        self.electricity_relevant.drop(self.electricity_relevant[\n                pd.to_datetime(self.electricity_relevant['date']) <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)\n    \n    def add_electricity(self, data):\n        return self.add_electricity_daylag(data, 0)\n    \n    def add_electricity_daylag(self, data, days):\n        col_names = [f'electricity_price__lag_{days}d']\n        if days < len(self.electricity_relevant):\n            data[col_names[0]] = self.electricity_relevant.iloc[-days-1, :]['electricity_price']\n        else:\n            data[col_names[0]] = np.nan\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.480450Z","iopub.execute_input":"2024-01-31T23:21:14.480789Z","iopub.status.idle":"2024-01-31T23:21:14.495858Z","shell.execute_reply.started":"2024-01-31T23:21:14.480761Z","shell.execute_reply":"2024-01-31T23:21:14.494713Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TargetCompiler:\n    def __init__(self, days_to_save=30):\n        self.days_to_save = days_to_save\n        self.target_relevant = pd.DataFrame(columns=['county', 'is_business', 'product_type', 'target', 'is_consumption', 'datetime'])\n    \n    def update(self, previous_target_block):\n        self.target_relevant = pd.concat([self.target_relevant, previous_target_block])\n        date_value = pd.to_datetime(previous_target_block['datetime'].iloc[0])\n        self.target_relevant.drop(self.target_relevant[\n            self.target_relevant['datetime'] <= date_value - pd.Timedelta(days=self.days_to_save)\n        ].index, axis=0, inplace=True)\n    \n    def add_agg_target_daylag(self, data, days, agg_trough=[['county'], ['product_type'], ['county', 'product_type']], agg_fns=['mean', 'sum', 'max', 'min', 'std']):\n        assert days > 1\n        ALL_GROUP_COLS = ['county', 'product_type', 'is_business', 'is_consumption', 'datetime']\n        col_name = f'target__lag_{days}d'\n        target_block = self.target_relevant.copy()\n        target_block['datetime'] = target_block['datetime'] + pd.Timedelta(days=days)\n        if col_name not in data.columns:\n            target_block[col_name] = target_block['target']\n        for by in agg_trough:\n            grby_cols = list(set(ALL_GROUP_COLS) - set(by))\n            for func in agg_fns:\n                new_name = col_name + '__all_' + '_'.join(by) + '_' + func\n                target_block[new_name] = target_block.groupby(grby_cols)['target'].transform(func)\n        target_block = target_block.drop(columns=['target'])\n        data = pd.merge(data, target_block, on=['county', 'product_type', 'is_business', 'is_consumption', 'datetime'], how='left') #  equal is_consumption\n        return data\n\n    def add_inverted_target_daylag(self, data, days):\n        assert days > 1\n        target_block = self.target_relevant.copy()\n        target_block['datetime'] = target_block['datetime'] + pd.Timedelta(days=days)\n        target_block = target_block.rename(columns={'target': f'inv_target__lag_{days}d'})\n        target_block['is_consumption'] = (target_block['is_consumption'] + 1) % 2\n        data = pd.merge(data, target_block, on=['county', 'product_type', 'is_business', 'is_consumption', 'datetime'], how='left') #  invert is_consumption\n        return data\n    \n    def add_target_return(self, data, nominator_daylag, denominator_daylag):\n        assert nominator_daylag > 1 and denominator_daylag > nominator_daylag\n        nominator_block = self.target_relevant.copy()\n        nominator_block['datetime'] = nominator_block['datetime'] + pd.Timedelta(days=nominator_daylag)\n        nominator_block.rename(columns={'target': 'target_nominator'}, inplace=True)\n        denominator_block = self.target_relevant.copy()\n        denominator_block['datetime'] = denominator_block['datetime'] + pd.Timedelta(days=denominator_daylag)\n        denominator_block.rename(columns={'target': 'target_denominator'}, inplace=True)\n        data = pd.merge(data, nominator_block, on=['county', 'product_type', 'is_business', 'is_consumption', 'datetime'], how='left')\n        data['target_nominator'] = data['target_nominator'].fillna(value=0)\n        data = pd.merge(data, denominator_block, on=['county', 'product_type', 'is_business', 'is_consumption', 'datetime'], how='left')\n        data['target_denominator'] = data['target_denominator'].fillna(value=0)\n        data[f'target_return_{nominator_daylag}_{denominator_daylag}'] = data['target_nominator'] / (data['target_denominator'] + 1e-3)\n        data = data.drop(columns=['target_nominator', 'target_denominator'])\n        return data\n        \n    def add_target_sma_ema(self, data, day_period):\n        assert day_period > 1\n        target_block = self.target_relevant.copy()\n        target_block['datetime'] = target_block['datetime'] + pd.Timedelta(days=2)\n        sma_block = data[['county', 'product_type', 'is_business', 'is_consumption', 'datetime']]\n        if len(target_block) > 0:\n#             day_period = 0\n            for day in range(day_period):\n                target_block['datetime'] = target_block['datetime'] + pd.Timedelta(days=1)\n                sma_block = pd.merge(sma_block, target_block, on=['county', 'product_type', 'is_business', 'is_consumption', 'datetime'], how='left')\n                sma_block.rename(columns={'target': f'target_day_{day}'}, inplace=True)\n                sma_block[f'target_day_{day}'] = sma_block[f'target_day_{day}'].fillna(value=0)\n        \n        sma_block['target_sma'] = 0\n        sma_block['target_ema'] = 0\n        alpha = 2 / (1 + day_period)\n        if len(target_block) > 0:\n            for day in range(day_period):\n                sma_block['target_sma'] = sma_block['target_sma'] + sma_block[f'target_day_{day}']\n                sma_block['target_ema'] = (1 - alpha) * sma_block['target_ema'] + alpha * sma_block[f'target_day_{day}']\n\n        data[f'target__sma_{day_period}'] = list(sma_block['target_sma'] / (day_period + 1e-9))\n        data[f'target__ema_{day_period}'] = list(sma_block['target_ema'] / 1)  # div by 1 -> int to float in case of day_period=0\n        return data\n    \n#     def add_target_daylag_daymean(self, data, days):\n#         self.update_counter += 1\n#         col_names = [f'target__lag_{days}d', f'inv_target__lag_{days}d']\n#         target_block = self.target_relevant.copy()\n#         target_block['datetime'] = target_block['datetime'] + pd.Timedelta(days=days)\n#         target_block = target_block.rename(columns={'target': col_names[0]})\n#         data = pd.merge(data, target_block, on=['county', 'product_type', 'is_business', 'is_consumption', 'datetime'], how='left') #  equal is_consumption\n#         target_block['is_consumption'] = (target_block['is_consumption'] + 1) % 2\n#         target_block = target_block.rename(columns={col_names[0]: col_names[1]})\n#         data = pd.merge(data, target_block, on=['county', 'product_type', 'is_business', 'is_consumption', 'datetime'], how='left') #  invert is_consumption\n#         return data","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.497442Z","iopub.execute_input":"2024-01-31T23:21:14.498511Z","iopub.status.idle":"2024-01-31T23:21:14.521100Z","shell.execute_reply.started":"2024-01-31T23:21:14.498458Z","shell.execute_reply":"2024-01-31T23:21:14.519807Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Compiler:\n    def __init__(self, days_to_save=30):\n        self.days_to_save = days_to_save\n        self.forecasted_weather_compiler = ForecastedWeatherCompiler(days_to_save=days_to_save)\n        self.historical_weather_compiler = HistoricalWeatherCompiler(days_to_save=days_to_save)\n        self.client_compiler = ClientCompiler(days_to_save=days_to_save)\n        self.gas_compiler = GasCompiler(days_to_save=days_to_save)\n        self.electricity_compiler = ElectricityCompiler(days_to_save=days_to_save)\n        self.target_compiler = TargetCompiler(days_to_save=days_to_save)\n        \n        self.holidays = holidays.country_holidays('EE', years=range(2021, 2026)).keys()\n        \n    def _check_valid(self, block):\n        if block is None:\n            return False\n        if len(block) == 0:\n            return False\n        return True\n        \n    def update(self, historical_weather_block=None, forecasted_weather_block=None,\n                     client_block=None,\n                     gas_block=None, electricity_block=None,\n                     previous_target=None):\n        if self._check_valid(forecasted_weather_block):\n            self.forecasted_weather_compiler.update(forecasted_weather_block)\n        if self._check_valid(historical_weather_block):\n            self.historical_weather_compiler.update(historical_weather_block)\n        if self._check_valid(client_block):\n            self.client_compiler.update(client_block)\n        if self._check_valid(gas_block):\n            self.gas_compiler.update(gas_block)\n        if self._check_valid(electricity_block):\n            self.electricity_compiler.update(electricity_block)\n        if self._check_valid(previous_target):\n            self.target_compiler.update(previous_target)\n        \n    def add_base_features(self, data):\n        data['datetime'] = pd.to_datetime(data['datetime'])\n        data['segment'] = data['county'].astype(int).astype(str) + '_' + \\\n                          data['is_business'].astype(int).astype(str) + '_' + \\\n                          data['product_type'].astype(int).astype(str) + '_' + \\\n                          data['is_consumption'].astype(int).astype(str)\n        data['hour'] = data['datetime'].apply(lambda x: x.hour)\n        data['hour_sin'] = np.sin(data['hour'] / 12 * np.pi)\n        data['hour_cos'] = np.cos(data['hour'] / 12 * np.pi)\n        data['day_of_week'] = data['datetime'].apply(lambda x: x.weekday())\n        data['month'] = data['datetime'].apply(lambda x: x.month)\n        data['month_sin'] = np.sin(data['month'] / 6 * np.pi)\n        data['month_cos'] = np.cos(data['month'] / 6 * np.pi)\n        data['is_weekend'] = data['day_of_week'].isin([5, 6])\n        data['is_holiday'] = data['datetime'].apply(lambda x: x.date()).isin(self.holidays)\n        data['day_of_year'] = data['datetime'].apply(lambda x: x.timetuple().tm_yday)\n        data['day_of_year_sin'] = np.sin(data['day_of_year'] / 183 * np.pi)\n        data['day_of_year_cos'] = np.cos(data['day_of_year'] / 183 * np.pi)\n\n        data = self.forecasted_weather_compiler.add_weather(data)\n        data = self.forecasted_weather_compiler.add_weather_allsta(data)\n        data = self.client_compiler.add_client(data)\n        data = self.gas_compiler.add_gas(data)\n        data = self.electricity_compiler.add_electricity(data)\n        \n        data['direct_solar_radiation__effective'] = data['direct_solar_radiation'] * data['installed_capacity'] / (data['temperature'] + 273.15)\n        data['direct_solar_radiation__eff_solar_ideal'] = data['direct_solar_radiation__effective'] * data['solar_angle_ideal_cos']\n        data['direct_solar_radiation__eff_solar'] = data['direct_solar_radiation__effective'] * data['solar_angle_hor_cos']\n        data['surface_solar_radiation_downwards__effective'] = data['surface_solar_radiation_downwards'] * data['installed_capacity'] / (data['temperature'] + 273.15)\n        return data\n    \n    def add_weather_client_daycombolag(self, data, days, feats=None):\n        suffix = f'__lag_{days}d'\n        suffix_mean = f'__meanlag_{days}d'\n        # data = self.forecasted_weather_compiler.add_weather_hourlag(data, days, feats)\n        data = self.historical_weather_compiler.add_weather_daylag(data, days, feats)\n        data = self.historical_weather_compiler.add_weather_daylag_daymean(data, days, feats)\n        data = self.client_compiler.add_client_daylag(data, days)\n        data['shortwave_radiation__effective' + suffix] = data['shortwave_radiation' + suffix] * data['installed_capacity' + suffix] / (data['temperature' + suffix] + 273.15)\n        data['direct_solar_radiation__effective' + suffix] = data['direct_solar_radiation' + suffix] * data['installed_capacity' + suffix] / (data['temperature' + suffix] + 273.15)\n        data['direct_solar_radiation__eff_solar_ideal' + suffix] = data['direct_solar_radiation__effective' + suffix] * data['solar_angle_ideal_cos' + suffix]\n        data['direct_solar_radiation__eff_solar' + suffix] = data['direct_solar_radiation__effective' + suffix] * data['solar_angle_hor_cos' + suffix]\n        \n        data['diffuse_radiation__effective' + suffix] = data['diffuse_radiation' + suffix] * data['installed_capacity' + suffix] / (data['temperature' + suffix] + 273.15)\n        data['shortwave_radiation__effective' + suffix_mean] = data['shortwave_radiation' + suffix_mean] * data['installed_capacity' + suffix] / (data['temperature' + suffix] + 273.15)\n        data['direct_solar_radiation__effective' + suffix_mean] = data['direct_solar_radiation' + suffix_mean] * data['installed_capacity' + suffix] / (data['temperature' + suffix] + 273.15)\n        data['diffuse_radiation__effective' + suffix_mean] = data['diffuse_radiation' + suffix_mean] * data['installed_capacity' + suffix] / (data['temperature' + suffix] + 273.15)\n        \n        return data\n    \n    def add_weather_client_hourcombolag(self, data, hours, feats=None):\n        suffix = f'__lag_{hours}h'\n        data = self.forecasted_weather_compiler.add_weather_hourlag(data, hours, feats)\n        data['direct_solar_radiation__effective' + suffix] = data['direct_solar_radiation' + suffix] * data['installed_capacity'] / (data['temperature' + suffix] + 273.15)\n        data['direct_solar_radiation__eff_solar_ideal' + suffix] = data['direct_solar_radiation__effective' + suffix] * data['solar_angle_ideal_cos' + suffix]\n        data['direct_solar_radiation__eff_solar' + suffix] = data['direct_solar_radiation__effective' + suffix] * data['solar_angle_hor_cos' + suffix]\n        data['surface_solar_radiation_downwards__effective' + suffix] = data['surface_solar_radiation_downwards' + suffix] * data['installed_capacity'] / (data['temperature' + suffix] + 273.15)\n        return data\n        \n    def add_lag_features(self, data): \n        data = self.forecasted_weather_compiler.add_weather_hourlag(data, 7*24)\n        data = self.forecasted_weather_compiler.add_weather_allsta_hourlag(data, 7*24)\n    \n#         data = self.historical_weather_compiler.add_weather_daylag(data, 2)\n        data = self.add_weather_client_daycombolag(data, 2)\n        data = self.historical_weather_compiler.add_weather_daylag(data, 7)\n        data = self.historical_weather_compiler.add_weather_allsta_daylag(data, 1)\n        data = self.historical_weather_compiler.add_weather_allsta_daylag(data, 2)\n        data = self.historical_weather_compiler.add_weather_allsta_daylag(data, 7)\n        \n        for lag in range(2, 15):\n            data = self.target_compiler.add_agg_target_daylag(data, lag, \n                                                              agg_trough=[], \n                                                              agg_fns=[])\n        for lag in [2, 3, 7, 14]:\n            data = self.target_compiler.add_agg_target_daylag(data, lag, \n                                                              agg_trough=[['product_type'], ['county', 'product_type']], \n                                                              agg_fns=['sum', 'std'])\n            data = self.target_compiler.add_inverted_target_daylag(data, lag)\n        \n        data = self.target_compiler.add_target_return(data, 2, 3)\n        data = self.target_compiler.add_target_return(data, 3, 10)\n        data = self.target_compiler.add_target_return(data, 2, 9)\n        data = self.target_compiler.add_target_return(data, 7, 14)\n        \n        data[f'target__all_product_type_sum__return_2d_3d'] = data['target__lag_2d__all_product_type_sum'] / (data['target__lag_3d__all_product_type_sum'] + 1e-3)\n        data[f'target__all_product_type_sum__return_7d_14d'] = data['target__lag_7d__all_product_type_sum'] / (data['target__lag_14d__all_product_type_sum'] + 1e-3)\n        data[f'target__all_county_product_type_sum__return_2d_3d'] = data['target__lag_2d__all_county_product_type_sum'] / (data['target__lag_3d__all_county_product_type_sum'] + 1e-3)\n        data[f'target__all_county_product_type_sum__return_7d_14d'] = data['target__lag_7d__all_county_product_type_sum'] / (data['target__lag_14d__all_county_product_type_sum'] + 1e-3)\n        \n        target_lags = [f'target__lag_{lag}d' for lag in range(2, 15)]\n        data['target_mean_14'] = data[target_lags].mean(axis=1)\n        data['target_std_14'] = data[target_lags].std(axis=1)\n        \n        target_lags = [f'target__lag_{lag}d' for lag in range(2, 8)]\n        data['target_mean_7'] = data[target_lags].mean(axis=1)\n        data['target_std_7'] = data[target_lags].std(axis=1)\n        \n        data = self.target_compiler.add_target_sma_ema(data, 7)\n        \n        for col in ['temperature', \n                    'dewpoint', \n                    'u_wind_component', \n                    'v_wind_component', \n            ]:\n                data[f\"{col}_diff_1\"] = data.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(1)\n        \n        return data\n    \n    def generate_features(self, data):\n        data = self.add_base_features(data)\n        data = self.add_lag_features(data)\n        segment_columns = ['county', 'is_business', 'product_type', 'is_consumption']\n        data[segment_columns] = data[segment_columns].astype(int)\n        data['segment'] = data['segment'].astype('category')\n        return data","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.523813Z","iopub.execute_input":"2024-01-31T23:21:14.524333Z","iopub.status.idle":"2024-01-31T23:21:14.557171Z","shell.execute_reply.started":"2024-01-31T23:21:14.524271Z","shell.execute_reply":"2024-01-31T23:21:14.555843Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if IS_GENFEATS:\n    compiler = Compiler(days_to_save=18)\n    df_feats_full = []\n    blocks = df_data['data_block_id'].unique()\n    blocks.sort()\n    if IS_SUBMIT:\n        blocks = blocks[-24:-4]\n    for b in tqdm(blocks):\n        df_block = df_data.copy()\n        df_block = df_block[df_block['data_block_id'] == b]\n        df_block = df_block.drop(columns=['row_id', 'prediction_unit_id', 'target'])\n\n        df_block_target = df_data.copy()\n        df_block_target = df_block_target[df_block_target['data_block_id'] == b-2]\n        df_block_target['datetime'] = pd.to_datetime(df_block_target['datetime'])\n        df_block_target = df_block_target.drop(columns=['data_block_id', 'row_id', 'prediction_unit_id'])\n\n        df_client_block = df_client[df_client['data_block_id'] == b]\n        df_gas_prices_block = df_gas_prices[df_gas_prices['data_block_id'] == b]\n        df_electricity_prices_block = df_electricity_prices[df_electricity_prices['data_block_id'] == b]\n        df_forecast_weather_block = df_forecast_weather[df_forecast_weather['data_block_id'] == b]\n        df_historical_weather_block = df_historical_weather[df_historical_weather['data_block_id'] == b]\n\n        compiler.update(df_historical_weather_block, df_forecast_weather_block, \n                        df_client_block, df_gas_prices_block, df_electricity_prices_block, \n                        df_block_target)\n\n        df_feats_full.append(compiler.generate_features(df_block))\n\n    df_feats = pd.concat(df_feats_full).reset_index(drop=True)\n\n    df_target = df_data.copy()\n    df_target['datetime'] = pd.to_datetime(df_data['datetime'])\n    df_target = df_target.drop(columns=['data_block_id', 'row_id', 'prediction_unit_id'])\n    df_feats_target = pd.merge(df_feats, df_target, on=['county', 'product_type', 'is_business', \n                                                        'is_consumption', 'datetime'], how='left')\n    df_feats_target = df_feats_target.dropna(subset=['target'])\n    # df_feats_target.fillna(0, inplace=True) ???\n\n    del df_feats_full, df_target, df_feats\n    df_feats_full, df_target, df_feats = [], pd.DataFrame(), pd.DataFrame()\n    gc.collect()\nelse:\n    df_feats_target = pd.read_csv('/kaggle/input/enefit-dataset-187feats/df_187feats_target.csv', \n                                 index_col = [0])\n    df_feats_target['datetime'] = pd.to_datetime(df_feats_target['datetime'])\n\ndf_feats_target['segment'] = df_feats_target['segment'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:14.558686Z","iopub.execute_input":"2024-01-31T23:21:14.559048Z","iopub.status.idle":"2024-01-31T23:21:39.625373Z","shell.execute_reply.started":"2024-01-31T23:21:14.559019Z","shell.execute_reply":"2024-01-31T23:21:39.624583Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffecfa7f0784821a2f99386c951bd1b"}},"metadata":{}}]},{"cell_type":"code","source":"# reduce mem usage\nif not IS_SUBMIT:\n    cols = list(df_feats_target.columns)\n    cols.remove('datetime')\n    cols.remove('segment')\n    df_feats_target[cols] = df_feats_target[cols].apply(pd.to_numeric, downcast='float')\n    df_feats_target[cols] = df_feats_target[cols].apply(pd.to_numeric, downcast='integer')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:39.626464Z","iopub.execute_input":"2024-01-31T23:21:39.627092Z","iopub.status.idle":"2024-01-31T23:21:39.633579Z","shell.execute_reply.started":"2024-01-31T23:21:39.627058Z","shell.execute_reply":"2024-01-31T23:21:39.632545Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"del df_forecast_weather, df_historical_weather\ndf_forecast_weather, df_historical_weather = pd.DataFrame(), pd.DataFrame()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:39.635090Z","iopub.execute_input":"2024-01-31T23:21:39.635433Z","iopub.status.idle":"2024-01-31T23:21:39.810882Z","shell.execute_reply.started":"2024-01-31T23:21:39.635408Z","shell.execute_reply":"2024-01-31T23:21:39.809465Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"## LightGBMs","metadata":{}},{"cell_type":"code","source":"model_save_path = f'enefit-final'\nmodel_load_paths = [f'/kaggle/input/enefit-final/']\nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\nmodels = dict()","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:39.812191Z","iopub.execute_input":"2024-01-31T23:21:39.812504Z","iopub.status.idle":"2024-01-31T23:21:39.823288Z","shell.execute_reply.started":"2024-01-31T23:21:39.812477Z","shell.execute_reply":"2024-01-31T23:21:39.821871Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Lgbms division by is_consumption","metadata":{}},{"cell_type":"code","source":"if IS_CONS_MODEL:\n    cat_features = ['segment', 'hour','day_of_week', 'month','is_weekend','is_holiday', \n                    'county', 'is_business', 'product_type']\n    for is_cons in [0,1]:\n        name = f\"cons-{is_cons}\"\n        models[name] = []\n        for seed in [0, 1, 2, 3, 4, 42]:\n            if IS_TRAIN:\n                print(f'Train. {name}')\n                if IS_OFFLINE or K_FOLD:\n                    n_estimators = 4000\n                else:\n                    if is_cons == 0:\n                        n_estimators = 2300\n                    elif is_cons == 1:\n                        n_estimators = 1700\n                lgb_params = {\n                        \"random_state\": seed,\n                        \"objective\": \"mae\",\n                        \"n_estimators\": n_estimators,\n                        \"num_leaves\": 256,\n                        \"subsample\": 0.6, #0.8,\n                        \"colsample_bynode\": 0.6, #0.8,\n                        \"colsample_bytree\": 0.9,\n                        \"learning_rate\": 0.02,\n                        'max_depth': 10,\n                        \"n_jobs\": 4,\n                        \"device\": \"gpu\",\n                        \"verbose\": -1,\n                        \"importance_type\": \"gain\",\n                }\n                model = lgb.LGBMRegressor(**lgb_params)\n\n                mask = (df_feats_target['is_consumption'].astype(int) == is_cons)\n                df = df_feats_target[mask]\n                df = df.drop(['is_consumption'], axis=1)\n\n                if IS_OFFLINE:\n                    X_train = df[df['data_block_id'] <= split_data_block].drop(['data_block_id', 'datetime', 'target'], axis=1)\n                    y_train = df[df['data_block_id'] <= split_data_block]['target']\n                    X_valid = df[df['data_block_id'] > split_data_block].drop(['data_block_id', 'datetime', 'target'], axis=1)\n                    y_valid = df[df['data_block_id'] > split_data_block]['target']\n                    gc.collect()\n                    model.fit(X_train, y_train,\n                        eval_set=[(X_valid, y_valid)],\n                        categorical_feature=cat_features,\n                        callbacks=[lgb.callback.early_stopping(stopping_rounds=100),\n                                   lgb.callback.log_evaluation(period=100)\n                                  ],\n                    )\n\n                    model_filename = os.path.join(model_save_path, f'{mode}_{name}_seed-{seed}.txt')\n                    model.booster_.save_model(model_filename)\n                    models[name].append(model.booster_)\n                    print(f\"Final model saved to {model_filename}\")\n\n                elif not K_FOLD:  # not IS_OFFLINE\n                    X_train = df.drop(['data_block_id', 'datetime', 'target'], axis=1)\n                    y_train = df['target']\n                    gc.collect()\n                    model.fit(X_train, y_train,\n                        categorical_feature=cat_features,\n                    )\n\n                    model_filename = os.path.join(model_save_path, f'{mode}_{name}_seed-{seed}.txt')\n                    model.booster_.save_model(model_filename)\n                    print(f\"Final model saved to {model_filename}\")\n                    models[name].append(model.booster_)\n\n                elif K_FOLD:\n                    block_ids = df['data_block_id'].values\n                    df_train = df.drop(['data_block_id', 'datetime'], axis=1)\n                    X_train, y_train = df_train.drop(['target'], axis=1), df_train['target']\n                    del df_train\n                    gc.collect()\n                    num_folds = 10\n                    fold_size = 638 // num_folds\n                    for i in range(5):\n                        start = i * fold_size\n                        end = start + fold_size\n                        test_indices = (block_ids >= start) & (block_ids < end)\n                        model = lgb.LGBMRegressor(**lgb_params)\n                        model.fit(\n                            X_train[~test_indices], y_train[~test_indices],\n                            eval_set=[(X_train[test_indices], y_train[test_indices])],\n                            categorical_feature=cat_features,\n                            callbacks=[\n                                lgb.callback.early_stopping(stopping_rounds=100),\n                                lgb.callback.log_evaluation(period=100),\n                            ],\n                        )\n                        model_filename = os.path.join(model_save_path, f'{name}_10folds-{i+1}_seed-{seed}.txt')\n                        model.booster_.save_model(model_filename)\n                        print(f\"Model for fold {i+1} saved to {model_filename}\")\n                        models[name].append(model.booster_)\n\n            else:  # not IS_TRAIN\n                for model_load_path in model_load_paths:\n                    model_load_file = os.path.join(model_load_path, f'{mode}_{name}_seed-{seed}.txt')\n                    if os.path.exists(model_load_file):\n                        models[name].append(lgb.Booster(model_file=model_load_file))\n                    if K_FOLD:\n                        for i in range(5):\n                            model_load_file = os.path.join(model_load_path, f'{name}_10folds-{i+1}_seed-{seed}.txt')\n                            if os.path.exists(model_load_file):\n                                models[name].append(lgb.Booster(model_file=model_load_file))\n        print(f'{len(models[name])} models are ready')                    \n    print(f'{len(models)} groups of models are ready')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:39.824530Z","iopub.execute_input":"2024-01-31T23:21:39.824947Z","iopub.status.idle":"2024-01-31T23:21:39.845543Z","shell.execute_reply.started":"2024-01-31T23:21:39.824924Z","shell.execute_reply":"2024-01-31T23:21:39.844256Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"if not IS_SUBMIT:\n    for name in ['cons-0', 'cons-1']:\n        model = models[name][-1]\n        fea_imp = pd.DataFrame({'Feature Id':model.feature_name(), 'Importances':model.feature_importance()})\n        feature_importances_all = fea_imp.loc[fea_imp['Importances'] > 0].sort_values(by=['Importances'], ascending = False)\n        feature_importances_all.to_csv(f'fea_imp_{name}.csv')\n        plt.figure(figsize=(12, 12))\n        sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feature_importances_all[:30])\n        plt.title(f'LightGBM[{name}] features importance ')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:39.847461Z","iopub.execute_input":"2024-01-31T23:21:39.847892Z","iopub.status.idle":"2024-01-31T23:21:39.862788Z","shell.execute_reply.started":"2024-01-31T23:21:39.847856Z","shell.execute_reply":"2024-01-31T23:21:39.861834Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Diff lgbms division by is_consumption","metadata":{}},{"cell_type":"code","source":"if IS_CONS_DIFF_MODEL:\n    cat_features = ['segment', 'hour','day_of_week', 'month','is_weekend','is_holiday', \n                    'county', 'is_business', 'product_type']\n    for is_cons in [0,1]:\n        name = f\"diff-cons-{is_cons}\"\n        models[name] = []\n        for seed in [0, 1, 2, 3, 4, 42]:\n            if IS_TRAIN:\n                print(f'Train. {name}')\n                if IS_OFFLINE or K_FOLD:\n                    n_estimators = 4000\n                else:\n                    if is_cons == 0:\n                        n_estimators = 1700\n                    elif is_cons == 1:\n                        n_estimators = 1600\n                lgb_params = {\n                        \"random_state\": seed,\n                        \"objective\": \"mae\",\n                        \"n_estimators\": n_estimators,\n                        \"num_leaves\": 256,\n                        \"subsample\": 0.6,\n                        \"colsample_bynode\": 0.6,\n                        \"colsample_bytree\": 0.9,\n                        \"learning_rate\": 0.038,\n                        'max_depth': 10,\n                        \"n_jobs\": 4,\n                        \"device\": \"gpu\",\n                        \"verbose\": -1,\n                        \"importance_type\": \"gain\",\n                }\n                model = lgb.LGBMRegressor(**lgb_params)\n\n                mask = (df_feats_target['is_consumption'].astype(int) == is_cons)\n                df = df_feats_target[mask]\n                df = df.drop(['is_consumption'], axis=1)\n\n                if IS_OFFLINE:\n                    df_train = df[df['data_block_id'] <= split_data_block].drop(columns=['data_block_id', 'datetime'])\n                    df_valid = df[df['data_block_id'] > split_data_block].drop(columns=['data_block_id', 'datetime'])\n                    gc.collect()\n                    model.fit(df_train.drop(columns=['target']), \n                              df_train['target'] - df_train['target__lag_2d'].fillna(0),\n                        eval_set=[(\n                            df_valid.drop(columns=['target']), \n                            df_valid['target'] - df_valid['target__lag_2d'].fillna(0)\n                        )],\n                        categorical_feature=cat_features,\n                        callbacks=[lgb.callback.early_stopping(stopping_rounds=100),\n                                   lgb.callback.log_evaluation(period=100)\n                                  ],\n                    )\n\n                    model_filename = os.path.join(model_save_path, f'{mode}_{name}_seed-{seed}.txt')\n                    model.booster_.save_model(model_filename)\n                    models[name].append(model.booster_)\n                    print(f\"Final model saved to {model_filename}\")\n\n                elif not K_FOLD:  # not IS_OFFLINE\n                    df_train = df.drop(columns=['data_block_id', 'datetime'])\n                    gc.collect()\n                    model.fit(df_train.drop(columns=['target']), \n                              df_train['target'] - df_train['target__lag_2d'].fillna(0),\n                        categorical_feature=cat_features,\n                    )\n\n                    model_filename = os.path.join(model_save_path, f'{mode}_{name}_seed-{seed}.txt')\n                    model.booster_.save_model(model_filename)\n                    print(f\"Final model saved to {model_filename}\")\n                    models[name].append(model.booster_)\n\n            else:  # not IS_TRAIN\n                for model_load_path in model_load_paths:\n                    model_load_file = os.path.join(model_load_path, f'{mode}_{name}_seed-{seed}.txt')\n                    if os.path.exists(model_load_file):\n                        models[name].append(lgb.Booster(model_file=model_load_file))\n                    \n        print(f'{len(models[name])} models are ready')                    \n    print(f'{len(models)} groups of models are ready')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:39.863747Z","iopub.execute_input":"2024-01-31T23:21:39.864861Z","iopub.status.idle":"2024-01-31T23:21:46.140338Z","shell.execute_reply.started":"2024-01-31T23:21:39.864813Z","shell.execute_reply":"2024-01-31T23:21:46.139242Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"6 models are ready\n6 models are ready\n2 groups of models are ready\n","output_type":"stream"}]},{"cell_type":"code","source":"if not IS_SUBMIT:\n    for name in ['diff-cons-0', 'diff-cons-1']:\n        model = models[name][-1]\n        fea_imp = pd.DataFrame({'Feature Id':model.feature_name(), 'Importances':model.feature_importance()})\n        feature_importances_all = fea_imp.loc[fea_imp['Importances'] > 0].sort_values(by=['Importances'], ascending = False)\n        feature_importances_all.to_csv(f'fea_imp_{name}.csv')\n        plt.figure(figsize=(12, 12))\n        sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feature_importances_all[:30])\n        plt.title(f'LightGBM[{name}] features importance ')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.141418Z","iopub.execute_input":"2024-01-31T23:21:46.141693Z","iopub.status.idle":"2024-01-31T23:21:46.150197Z","shell.execute_reply.started":"2024-01-31T23:21:46.141669Z","shell.execute_reply":"2024-01-31T23:21:46.148372Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Diff 7days lgbms division by is_consumption","metadata":{}},{"cell_type":"code","source":"if IS_CONS_DIFF7_MODEL:\n    cat_features = ['segment', 'hour','day_of_week', 'month','is_weekend','is_holiday', \n                    'county', 'is_business', 'product_type']\n    for is_cons in [0,1]:\n        name = f\"diff7-cons-{is_cons}\"\n        models[name] = []\n        for seed in [0, 1, 2, 3, 4, 42]:\n            if IS_TRAIN:\n                print(f'Train. {name}')\n                if IS_OFFLINE or K_FOLD:\n                    n_estimators = 4000\n                else:\n                    if is_cons == 0:\n                        n_estimators = 3000\n                    elif is_cons == 1:\n                        n_estimators = 1200\n                lgb_params = {\n                        \"random_state\": seed,\n                        \"objective\": \"mae\",\n                        \"n_estimators\": n_estimators,\n                        \"num_leaves\": 256,\n                        \"subsample\": 0.6,\n                        \"colsample_bynode\": 0.6,\n                        \"colsample_bytree\": 0.9,\n                        \"learning_rate\": 0.03,\n                        'max_depth': 10,\n                        \"n_jobs\": 4,\n                        \"device\": \"gpu\",\n                        \"verbose\": -1,\n                        \"importance_type\": \"gain\",\n                }\n                model = lgb.LGBMRegressor(**lgb_params)\n\n                mask = (df_feats_target['is_consumption'].astype(int) == is_cons)\n                df = df_feats_target[mask]\n                df = df.drop(['is_consumption'], axis=1)\n\n                if IS_OFFLINE:\n                    df_train = df[df['data_block_id'] <= split_data_block].drop(columns=['data_block_id', 'datetime'])\n                    df_valid = df[df['data_block_id'] > split_data_block].drop(columns=['data_block_id', 'datetime'])\n                    gc.collect()\n                    model.fit(df_train.drop(columns=['target']), \n                              df_train['target'] - df_train['target__lag_7d'].fillna(0),\n                        eval_set=[(\n                            df_valid.drop(columns=['target']), \n                            df_valid['target'] - df_valid['target__lag_7d'].fillna(0)\n                        )],\n                        categorical_feature=cat_features,\n                        callbacks=[lgb.callback.early_stopping(stopping_rounds=100),\n                                   lgb.callback.log_evaluation(period=100)\n                                  ],\n                    )\n\n                    model_filename = os.path.join(model_save_path, f'{mode}_{name}_seed-{seed}.txt')\n                    model.booster_.save_model(model_filename)\n                    models[name].append(model.booster_)\n                    print(f\"Final model saved to {model_filename}\")\n\n                elif not K_FOLD:  # not IS_OFFLINE\n                    df_train = df.drop(columns=['data_block_id', 'datetime'])\n                    gc.collect()\n                    model.fit(df_train.drop(columns=['target']), \n                              df_train['target'] - df_train['target__lag_7d'].fillna(0),\n                        categorical_feature=cat_features,\n                    )\n\n                    model_filename = os.path.join(model_save_path, f'{mode}_{name}_seed-{seed}.txt')\n                    model.booster_.save_model(model_filename)\n                    print(f\"Final model saved to {model_filename}\")\n                    models[name].append(model.booster_)\n\n            else:  # not IS_TRAIN\n                for model_load_path in model_load_paths:\n                    model_load_file = os.path.join(model_load_path, f'{mode}_{name}_seed-{seed}.txt')\n                    if os.path.exists(model_load_file):\n                        models[name].append(lgb.Booster(model_file=model_load_file))\n                    \n        print(f'{len(models[name])} models are ready')                    \n    print(f'{len(models)} groups of models are ready')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.152008Z","iopub.execute_input":"2024-01-31T23:21:46.152377Z","iopub.status.idle":"2024-01-31T23:21:46.171838Z","shell.execute_reply.started":"2024-01-31T23:21:46.152346Z","shell.execute_reply":"2024-01-31T23:21:46.170436Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"if not IS_SUBMIT:\n    for name in ['diff7-cons-0', 'diff7-cons-1']:\n        model = models[name][-1]\n        fea_imp = pd.DataFrame({'Feature Id':model.feature_name(), 'Importances':model.feature_importance()})\n        feature_importances_all = fea_imp.loc[fea_imp['Importances'] > 0].sort_values(by=['Importances'], ascending = False)\n        feature_importances_all.to_csv(f'fea_imp_{name}.csv')\n        plt.figure(figsize=(12, 12))\n        sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feature_importances_all[:30])\n        plt.title(f'LightGBM[{name}] features importance ')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.173215Z","iopub.execute_input":"2024-01-31T23:21:46.173547Z","iopub.status.idle":"2024-01-31T23:21:46.191882Z","shell.execute_reply.started":"2024-01-31T23:21:46.173521Z","shell.execute_reply":"2024-01-31T23:21:46.190497Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def predict(models, df_data, sample_prediction=None):\n    if sample_prediction is None:\n        sample_prediction = df_data.copy()\n        sample_prediction['target'] = 0\n        \n    # consumption division\n    if 'cons-0' in models.keys() and 'cons-1' in models.keys():\n        for is_cons in [0,1]:\n            name = f\"cons-{is_cons}\"\n            mask = (df_data['is_consumption'].astype(int) == is_cons)\n            df = df_data[mask]\n            if df.shape[0] == 0:\n                continue\n            df = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                          'target', 'prediction', 'currently_scored',\n                          'row_id', 'prediction_unit_id'],\n                         axis=1, errors='ignore')\n            weight = 1 / len(models[name])\n            prediction = np.zeros(df.shape[0])\n            for model in models[name]:\n                prediction += weight * np.nan_to_num(model.predict(df[model.feature_name()]))\n            sample_prediction.loc[mask.values, 'target'] += weights2name[name] * prediction.clip(0)\n    \n    # diff consumption division\n    if 'diff-cons-0' in models.keys() and 'diff-cons-1' in models.keys():\n        for is_cons in [0,1]:\n            name = f\"diff-cons-{is_cons}\"\n            mask = (df_data['is_consumption'].astype(int) == is_cons)\n            df = df_data[mask]\n            if df.shape[0] == 0:\n                continue\n            df = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                          'target', 'prediction', 'currently_scored',\n                          'row_id', 'prediction_unit_id'],\n                         axis=1, errors='ignore')\n            weight = 1 / len(models[name])\n            prediction = np.zeros(df.shape[0])\n            for model in models[name]:\n                prediction += weight * \\\n                              np.nan_to_num(model.predict(df[model.feature_name()]) + \\\n                                                     df['target__lag_2d'].fillna(0))\n            sample_prediction.loc[mask.values, 'target'] += weights2name[name] * prediction.clip(0)\n            \n    # diff 7days consumption division\n    if 'diff7-cons-0' in models.keys() and 'diff7-cons-1' in models.keys():\n        for is_cons in [0,1]:\n            name = f\"diff7-cons-{is_cons}\"\n            mask = (df_data['is_consumption'].astype(int) == is_cons)\n            df = df_data[mask]\n            if df.shape[0] == 0:\n                continue\n            df = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                          'target', 'prediction', 'currently_scored',\n                          'row_id', 'prediction_unit_id'],\n                         axis=1, errors='ignore')\n            weight = 1 / len(models[name])\n            prediction = np.zeros(df.shape[0])\n            for model in models[name]:\n                prediction += weight * \\\n                              np.nan_to_num(model.predict(df[model.feature_name()]) + \\\n                                                     df['target__lag_7d'].fillna(0))\n            sample_prediction.loc[mask.values, 'target'] += weights2name[name] * prediction.clip(0)\n        \n    return sample_prediction['target']","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.193398Z","iopub.execute_input":"2024-01-31T23:21:46.193683Z","iopub.status.idle":"2024-01-31T23:21:46.211495Z","shell.execute_reply.started":"2024-01-31T23:21:46.193660Z","shell.execute_reply":"2024-01-31T23:21:46.210601Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def refit(models, df_data, refit_decay=0.995):\n    # consumption division\n    if 'cons-0' in models.keys() and 'cons-1' in models.keys():\n        for is_cons in [0,1]:\n            name = f\"cons-{is_cons}\"\n            mask = (df_data['is_consumption'].astype(int) == is_cons)\n            df = df_data[mask]\n            if df.shape[0] == 0:\n                continue\n            df = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                          'prediction', 'currently_scored',\n                          'row_id', 'prediction_unit_id'],\n                         axis=1, errors='ignore')\n            y_train = df['target']\n            X_train = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                               'target', 'prediction', 'currently_scored'],\n                              axis=1, errors='ignore')\n            for i in range(len(models[name])):\n                models[name][i] = models[name][i].refit(X_train[models[name][i].feature_name()], \n                                                        y_train, \n                                                        decay_rate=refit_decay)\n    \n    # diff consumption division\n    if 'diff-cons-0' in models.keys() and 'diff-cons-1' in models.keys():\n        for is_cons in [0,1]:\n            name = f\"diff-cons-{is_cons}\"\n            mask = (df_data['is_consumption'].astype(int) == is_cons)\n            df = df_data[mask]\n            if df.shape[0] == 0:\n                continue\n            df = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                          'prediction', 'currently_scored',\n                          'row_id', 'prediction_unit_id'],\n                         axis=1, errors='ignore')\n            y_train = df['target'] - df['target__lag_2d'].fillna(0)\n            X_train = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                               'target', 'prediction', 'currently_scored'],\n                              axis=1, errors='ignore')\n            for i in range(len(models[name])):\n                models[name][i] = models[name][i].refit(X_train[models[name][i].feature_name()], \n                                                        y_train, \n                                                        decay_rate=refit_decay)\n                    \n    # diff 7days consumption division\n    if 'diff7-cons-0' in models.keys() and 'diff7-cons-1' in models.keys():\n        for is_cons in [0,1]:\n            name = f\"diff7-cons-{is_cons}\"\n            mask = (df_data['is_consumption'].astype(int) == is_cons)\n            df = df_data[mask]\n            if df.shape[0] == 0:\n                continue\n            df = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                          'prediction', 'currently_scored',\n                          'row_id', 'prediction_unit_id'],\n                         axis=1, errors='ignore')\n            y_train = df['target'] - df['target__lag_7d'].fillna(0)\n            X_train = df.drop(['is_consumption', 'data_block_id', 'datetime',\n                               'target', 'prediction', 'currently_scored'],\n                              axis=1, errors='ignore')\n            for i in range(len(models[name])):\n                models[name][i] = models[name][i].refit(X_train[models[name][i].feature_name()], \n                                                        y_train, \n                                                        decay_rate=refit_decay)\n    return models","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.212762Z","iopub.execute_input":"2024-01-31T23:21:46.213052Z","iopub.status.idle":"2024-01-31T23:21:46.230104Z","shell.execute_reply.started":"2024-01-31T23:21:46.213028Z","shell.execute_reply":"2024-01-31T23:21:46.228376Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"if IS_OFFLINE and not IS_SUBMIT:\n    df_valid1 = df_feats_target[df_feats_target['data_block_id'] > 500]\n    df_valid1['prediction'] = predict(models, df_valid1)\n    val_score = mean_absolute_error(df_valid1['target'], df_valid1['prediction'])\n    print(f'validation mae score = {val_score}')\n    # mean_absolute_error(df_valid1[df_valid1['data_block_id'] > 618]['target'], df_valid1[df_valid1['data_block_id'] > 618]['prediction'])    ","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.231620Z","iopub.execute_input":"2024-01-31T23:21:46.231937Z","iopub.status.idle":"2024-01-31T23:21:46.248586Z","shell.execute_reply.started":"2024-01-31T23:21:46.231914Z","shell.execute_reply":"2024-01-31T23:21:46.247234Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"for name in models.keys():\n    for i in range(len(models[name])):\n        models[name][i] = models[name][i].reset_parameter({'verbose':-1})","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.250256Z","iopub.execute_input":"2024-01-31T23:21:46.250660Z","iopub.status.idle":"2024-01-31T23:21:46.266821Z","shell.execute_reply.started":"2024-01-31T23:21:46.250629Z","shell.execute_reply":"2024-01-31T23:21:46.264797Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"if IS_OFFLINE and not IS_SUBMIT:\n    df_valid = df_feats_target[df_feats_target['data_block_id'] > split_data_block]\n    df_valid['prediction'] = None\n    days_to_refit = 28\n    min_block = np.min(df_valid['data_block_id'])\n    max_block = np.max(df_valid['data_block_id'])\n    for block in tqdm(range(min_block, max_block+1, days_to_refit)):\n        mask = (df_valid['data_block_id'] >= block) & \\\n               (df_valid['data_block_id'] < block + days_to_refit)\n        df_valid.loc[mask, 'prediction'] = predict(models, df_valid[mask])\n        mask = (df_valid['data_block_id'] >= block-2) & \\\n               (df_valid['data_block_id'] < block-2 + days_to_refit)\n        models = refit(models, df_valid[mask], refit_decay=0.996)\n    val_score = mean_absolute_error(df_valid['target'], df_valid['prediction'])\n    print(f'After refit: validation mae score = {val_score}')\n#     mean_absolute_error(df_valid[df_valid['data_block_id'] > 618]['target'], df_valid[df_valid['data_block_id'] > 618]['prediction'])","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.268485Z","iopub.execute_input":"2024-01-31T23:21:46.268792Z","iopub.status.idle":"2024-01-31T23:21:46.276257Z","shell.execute_reply.started":"2024-01-31T23:21:46.268766Z","shell.execute_reply":"2024-01-31T23:21:46.275197Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## API Submission","metadata":{}},{"cell_type":"code","source":"import enefit\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.280844Z","iopub.execute_input":"2024-01-31T23:21:46.281473Z","iopub.status.idle":"2024-01-31T23:21:46.313148Z","shell.execute_reply.started":"2024-01-31T23:21:46.281440Z","shell.execute_reply":"2024-01-31T23:21:46.311778Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def is_prediction_needed(test):\n    return not all(test['currently_scored'] == False)","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.314453Z","iopub.execute_input":"2024-01-31T23:21:46.314737Z","iopub.status.idle":"2024-01-31T23:21:46.319548Z","shell.execute_reply.started":"2024-01-31T23:21:46.314715Z","shell.execute_reply":"2024-01-31T23:21:46.318540Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"refit_decay = 2.\ndays_to_refit = 28\nassert days_to_refit > 5\n\ncur_data_block = 1000\ncache_data = pd.DataFrame()\ntarget_cols = ['target', 'county', 'is_business', 'product_type', \n               'is_consumption', 'datetime']\ncache_target = pd.DataFrame()\n\nfor (test, revealed_targets, client, \n     historical_weather, forecast_weather, \n     electricity_prices, gas_prices, \n     sample_prediction) in iter_test:\n    \n    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n    revealed_targets['datetime'] = pd.to_datetime(revealed_targets['datetime'])\n    revealed_targets = revealed_targets.drop(columns=['data_block_id', 'row_id', \n                                                      'prediction_unit_id'],\n                                             errors='ignore')\n    compiler.update(historical_weather, forecast_weather, \n                    client, gas_prices, electricity_prices, \n                    revealed_targets)\n    test_feats = compiler.generate_features(test)\n    \n    # == online training ==\n    if refit_decay < 1.:\n        revealed_targets = revealed_targets[target_cols]\n        revealed_targets['data_block_id'] = cur_data_block\n        cache_target = pd.concat([cache_target, revealed_targets], ignore_index=True, axis=0)\n        cache_target.drop(cache_target['data_block_id'] < cur_data_block - days_to_refit*2,\n                          axis=0, inplace=True)\n        test_feats['data_block_id'] = cur_data_block\n        cache_data = pd.concat([cache_data, test_feats], ignore_index=True, axis=0)\n        cache_data.drop(cache_data['data_block_id'] < cur_data_block - days_to_refit*2,\n                        axis=0, inplace=True)\n        if cur_data_block > 1000 + 5 and (cur_data_block-1000) % days_to_refit == 2:\n            cache_merged = pd.merge(cache_data, cache_target.drop(columns=['data_block_id']), \n                            on=['datetime', 'county', 'is_business', \n                                'product_type', 'is_consumption'], \n                            how='left')\n            refit_data = cache_merged[cache_merged['data_block_id'] >= cur_data_block - days_to_refit - 1]\n            refit_data = refit_data.dropna(subset=['target'])\n            models = refit(models, refit_data, refit_decay=refit_decay)\n    # ==-------------==\n    \n    # prediction\n    if is_prediction_needed(test):\n        sample_prediction['target'] = predict(models, test_feats, sample_prediction)\n    else:\n        sample_prediction['target'] = 0.0\n    \n    env.predict(sample_prediction)\n    \n    cur_data_block += 1","metadata":{"execution":{"iopub.status.busy":"2024-01-31T23:21:46.320605Z","iopub.execute_input":"2024-01-31T23:21:46.320891Z","iopub.status.idle":"2024-01-31T23:21:53.034594Z","shell.execute_reply.started":"2024-01-31T23:21:46.320859Z","shell.execute_reply":"2024-01-31T23:21:53.033505Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}